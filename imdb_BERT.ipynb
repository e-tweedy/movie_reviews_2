{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4bc657f",
   "metadata": {},
   "source": [
    "# Training a DistilBERT model to recognize movie review sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdace28",
   "metadata": {},
   "source": [
    "This was built following a tutorial in the wonderful book Machine Learning with PyTorch and Scikit-Learn by Raschka, Liu and Mirjalili (2022, Packt Publishing). In this notebook, we do the following:\n",
    "\n",
    "1. Use the Hugging Face transformers library (https://huggingface.co/docs/transformers/index) to load the pre-trained model DistilBertForSequenceClassification and tokenizer DistilBertTokenizerFast.  DistilBERT was proposed here: https://arxiv.org/abs/1910.01108\n",
    "2. We fine-tune the pre-trained model on the IMDb Large Movie Reviews Dataset: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "3. We eeploy the trained model as an interactive web app using the Gradio library (https://gradio.app/).\n",
    "Note that the Gradio app can be found by visiting my huggingface page:\n",
    "\n",
    "https://huggingface.co/spaces/etweedy/movie_review_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b2880",
   "metadata": {},
   "source": [
    "Import libraries and set gpu device and random seed for repeatability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b090ffbc-c985-45c4-a5bc-2de4c68f9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install torchtext==0.13.0\n",
    "! pip install torchdata==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea451f74-598f-4c45-9be1-551f2ba08a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed2fae73-d149-4947-a1f5-2dcd7e22fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministric=True\n",
    "torch.manual_seed(123)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NUM_EPOCHS=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b377860",
   "metadata": {},
   "source": [
    "Download and extract the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "595dcef2-cd49-4824-ba1c-e9d0cbdb1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\"https://github.com/rasbt/machine-learning-book/raw/main/ch08/movie_data.csv.gz\")\n",
    "filename = url.split(\"/\")[-1]\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    r = requests.get(url)\n",
    "    f.write(r.content)\n",
    "    \n",
    "with gzip.open('movie_data.csv.gz','rb') as f_in:\n",
    "    with open('movie_data.csv','wb') as f_out:\n",
    "        shutil.copyfileobj(f_in,f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d458acc0",
   "metadata": {},
   "source": [
    "Read csv into dataframe and split into training (70% of samples), validation (10% of samples), and test data (20% of samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6f3f73d-e442-45d6-b405-350021c0f179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c45f77-9301-4f3e-8023-0b7991b66d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "valid_texts = df.iloc[35000:40000]['review'].values\n",
    "valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "test_texts = df.iloc[40000:]['review'].values\n",
    "test_labels = df.iloc[40000:]['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda177aa",
   "metadata": {},
   "source": [
    "Load in the tokenizer and encode the three data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a07613c2-33b4-43e3-b4c0-7c9f020679c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_enc = tokenizer(list(train_texts),truncation=True,padding=True)\n",
    "valid_enc = tokenizer(list(valid_texts),truncation=True,padding=True)\n",
    "test_enc = tokenizer(list(test_texts),truncation=True,padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0dd4da",
   "metadata": {},
   "source": [
    "Create a custom Dataset class to hold encoded text and labels, initialize datasets and dataloaders for training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22f894b7-9a02-4cbb-b818-f1c751c1b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self,encodings,labels):\n",
    "        self.encodings=encodings\n",
    "        self.labels=labels\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item = {key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3713c477-a8e6-4cef-a15d-346b0b4bacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = IMDbDataset(train_enc,train_labels)\n",
    "valid_ds = IMDbDataset(valid_enc,valid_labels)\n",
    "test_ds = IMDbDataset(test_enc,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac65ecc7-1d72-4633-9d6c-7a4361f6ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds,batch_size=16,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds,batch_size=16,shuffle=True)\n",
    "test_dl = DataLoader(test_ds,batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd466edc",
   "metadata": {},
   "source": [
    "Load the pre-trained DistilBERT model, and declare optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d0de082-8586-47bc-9e49-ee5c985040d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57653500-1034-40c8-9471-bf68c5eefa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(),lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5abd0f",
   "metadata": {},
   "source": [
    "This function will compute accuracy as we train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6b709ef-226c-4ffa-961c-d4d6093ca041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model,dl,device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred,num_examples = 0,0\n",
    "\n",
    "        for batch_idx,batch in enumerate(dl):\n",
    "            # Collect the ids, mask, labels from dl and pass to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Evaluate the model, collect output logits, and calculate predictions\n",
    "            outputs = model(input_ids,attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            pred_labels = torch.argmax(logits,1)\n",
    "            \n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (pred_labels == labels).sum()\n",
    "            \n",
    "        # Compute percentage of accurate predictions\n",
    "        return correct_pred.float()/num_examples*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772cd23",
   "metadata": {},
   "source": [
    "Fine-tune the model and print scoring results as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba72bce-efa0-4e94-ad21-508a5f64fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 of 0003, batch 0000 of 2188 === Loss: 0.6800\n",
      "Epoch 0001 of 0003, batch 0250 of 2188 === Loss: 0.2488\n",
      "Epoch 0001 of 0003, batch 0500 of 2188 === Loss: 0.4501\n",
      "Epoch 0001 of 0003, batch 0750 of 2188 === Loss: 0.1309\n",
      "Epoch 0001 of 0003, batch 1000 of 2188 === Loss: 0.4273\n",
      "Epoch 0001 of 0003, batch 1250 of 2188 === Loss: 0.3193\n",
      "Epoch 0001 of 0003, batch 1500 of 2188 === Loss: 0.5093\n",
      "Epoch 0001 of 0003, batch 1750 of 2188 === Loss: 0.4583\n",
      "Epoch 0001 of 0003, batch 2000 of 2188 === Loss: 0.3154\n",
      "Training accuracy: 96.62 === Valid accuracy: 92.54\n",
      "Time elapsed: 18.81 min\n",
      "Epoch 0002 of 0003, batch 0000 of 2188 === Loss: 0.1179\n",
      "Epoch 0002 of 0003, batch 0250 of 2188 === Loss: 0.0136\n",
      "Epoch 0002 of 0003, batch 0500 of 2188 === Loss: 0.1435\n",
      "Epoch 0002 of 0003, batch 0750 of 2188 === Loss: 0.0454\n",
      "Epoch 0002 of 0003, batch 1000 of 2188 === Loss: 0.0768\n",
      "Epoch 0002 of 0003, batch 1250 of 2188 === Loss: 0.2802\n",
      "Epoch 0002 of 0003, batch 1500 of 2188 === Loss: 0.0200\n",
      "Epoch 0002 of 0003, batch 1750 of 2188 === Loss: 0.1257\n",
      "Epoch 0002 of 0003, batch 2000 of 2188 === Loss: 0.1308\n",
      "Training accuracy: 98.76 === Valid accuracy: 92.46\n",
      "Time elapsed: 37.67 min\n",
      "Epoch 0003 of 0003, batch 0000 of 2188 === Loss: 0.0074\n",
      "Epoch 0003 of 0003, batch 0250 of 2188 === Loss: 0.0039\n",
      "Epoch 0003 of 0003, batch 0500 of 2188 === Loss: 0.0611\n",
      "Epoch 0003 of 0003, batch 0750 of 2188 === Loss: 0.0306\n",
      "Epoch 0003 of 0003, batch 1000 of 2188 === Loss: 0.1513\n",
      "Epoch 0003 of 0003, batch 1250 of 2188 === Loss: 0.0014\n",
      "Epoch 0003 of 0003, batch 1500 of 2188 === Loss: 0.0020\n",
      "Epoch 0003 of 0003, batch 1750 of 2188 === Loss: 0.1905\n",
      "Epoch 0003 of 0003, batch 2000 of 2188 === Loss: 0.1545\n",
      "Training accuracy: 99.43 === Valid accuracy: 92.38\n",
      "Time elapsed: 56.56 min\n",
      "Total training time: 56.56 min\n",
      "Test accuracy: 93.06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dl):\n",
    "        # Collect the ids, mask, labels from dl and pass to device\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass of model\n",
    "        outputs = model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "        loss,logits = outputs['loss'],outputs['logits']\n",
    "        \n",
    "        # Backward pass of model\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # Print loss every 250 batches\n",
    "        if batch_idx % 250 == 0:\n",
    "            print(f'Epoch {epoch+1:04d} of {NUM_EPOCHS:04d}, batch {batch_idx:04d} of {len(train_dl):04d} === Loss: {loss:.4f}')\n",
    "    model.eval()\n",
    "    \n",
    "    # Print training and validation accuracy after each epoch\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'Training accuracy: {compute_accuracy(model,train_dl,DEVICE):.2f} === Valid accuracy: {compute_accuracy(model,valid_dl,DEVICE):.2f}')\n",
    "\n",
    "# Check accuracy of the fine-tuned model on the test set\n",
    "print(f'Test accuracy: {compute_accuracy(model,test_dl,DEVICE):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a240086",
   "metadata": {},
   "source": [
    "Create save directory and save our fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75ff008f-7992-4ff7-8210-f35848a2b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = './model_save/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f795dfc1-b1ca-473c-a6a8-278388e03915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json',\n",
       " './model_save/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Saving model to %s\" % output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c2133",
   "metadata": {},
   "source": [
    "Finally, we implement a little Gradio web app that we can use to interact with our model. The app will ask the user to input a movie review into a text entry box, and will return a prediction of 'Positive' or 'Negative' sentiment. The below code will generate a locally hosted app, but see the following blog post for a nice tutorial on deploying your web app on Hugging Face: https://huggingface.co/blog/gradio-spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c213111",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install gradio\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0feeb8a",
   "metadata": {},
   "source": [
    "Our app first needs a simple predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89e1eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    encoding=tokenizer(text,return_tensors='pt')\n",
    "    input_ids, attention_mask = encoding['input_ids'],encoding['attention_mask']\n",
    "    outputs = model(input_ids,attention_mask=attention_mask)\n",
    "    logits = outputs['logits']\n",
    "    pred_label = torch.argmax(logits,1)[0]\n",
    "    return 'Positive' if pred_label > 0.5 else 'Negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbec0a3",
   "metadata": {},
   "source": [
    "Then we initialize the Gradio app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd15b9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.12.0, however version 3.14.0 is available, please upgrade.\n",
      "--------\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"Write a movie review\"\n",
    "description = \"Enter a review for a movie you've seen.  This tool will try to guess whether your review is positive or negative.\"\n",
    "gr.Interface(fn=predict, \n",
    "             inputs=\"text\",\n",
    "             outputs=\"label\",\n",
    "             title = title,\n",
    "             description = description,\n",
    "              ).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969815c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
